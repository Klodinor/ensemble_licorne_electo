###################### Énoncé ######################

L’ensemble d’accéléromètre Électro



###################### Description ######################

Le but de ce projet est de permettre à des interprètes de jouer de la musique en temps réel en utilisant seulement les mouvements de la tête. 
Ceci permettra à des personnes à mobilité réduite (quadraplégique) de pouvoir faire de la musique. 

Dans le cadre de ce projet, je ferai  la conversion en données audio des données envoyées par les Arduinos. Ces Arduinos seront développés 
dans le cadre du cours en lutherie numérique. Pour ce faire, je ferai communiquer mes dispositifs avec un ordinateur grâce au langage de 
programmation Pyhton. Par la suite, grâce à de la synthèse sonore, je définirai un son propre à chaque instrument, jusqu’à un maximum de dix. 
Les timbres des divers instruments seront de l’un des deux types suivants : imitation de phénomènes sonores naturel (feu, vent, eau) ou 
purement synthétique. Pour finaliser la confection des timbres, je ferai du traitement sur le signal audio de chaque instrument en prévoyant 
une foule d’effets et plusieurs combinaisons de ceux-ci. L’activation des différents effets sera prédéfinie à l’intérieur de séquences. Il 
sera possible de lancer ces différentes séquences d’effets grâce à une interface graphique contrôlée via un contrôleur MIDI. Tout ce qui a 
trait à la création des timbres et des effets et de l’interface graphique, seront confectionnés grâce au langage de programmation Python 
et la librairie Pyo. 



###################### Analyse des besoins ######################

- Dispositif de "licorne" : comprend des accéléromètres rattachés à un Arduino et fixés à la tête des interprètes.  

- Conversion en données audio du flux de données envoyé à l’ordinateur par les Arduinos (Ces deux premières étapes seront réalisées dans le 
cadre du cours de lutherie numérique).

- Création d’instruments personnalisés à l’aide de sons de synthèse.

- Création et agencement de différents effets venant enrichir et diversifier le timbre des divers instruments (par exemple : pitch shifter, 
distorsion, réverbération, délai, etc).  

- Interface graphique : contrôle des volumes, gestion des séquences/effets, gestion du timbre initial propre à chaque interprète via un 
contrôleur MIDI.



###################### Acquisition de connaissances ######################

- Recherche sur la création de son de synthèse de correspondance "naturelle" et synthétique : Andy Farnell - Designing Sound

- Communication entre protocole MIDI et Python/Pyo : Chapitre sur les contrôleurs externes dans les notes de cours de programmation musical 
en langage python & https://docs.wxpython.org/ 

- Apprentissage de wxpython afin de créer l’interface graphique : https://docs.wxpython.org/, https://extras.wxpython.org/wxPython4/extras/4.0.4/, 
ainsi que les notes de cours portant sur ce sujet. 

- Approfondir mes connaissances avec le langage Python et la librairie Pyo : Note de cours de programmation musical en langage Python & API 
du langage lui-même https://docs.python.org/3/library/ 



###################### Modèle ######################

Ma première source d’inspiration dans la réalisation de ce projet est l’éponge de Martin Marier. J’en suis inspiré tant par les sonorités que 
par la méthode dont on en obtient des sons. Les sons produits par l’éponge jouent en continu et sont modifiés par le déplacement et les torsions 
appliqués à l’instrument. Ces différents mouvements sont détectés grâce à des accéléromètres. 

Tout comme avec l’éponge, il sera possible grâce au dispositif fixé à la tête des participants de faire des mouvements de très faible envergure 
afin de modifier le son de l’instrument et les paramètres des effets rattachés à celui-ci. Par exemple, on pourrait venir modifier le gain d’une 
distorsion, le temps d’une réverbération, le degré de dissonance de la note jouée, etc. Plus la distance sera grande entre la position actuelle 
du dispositif par rapport à son axe de calibration, plus la modification des paramètres sera accentuée. Cependant, contrairement à l’éponge de 
Martin Marier où des boutons sont situés sous l’instrument afin de changer la nature du timbre, la note jouée ou encore les effets appliqués, 
ici ces opérations seront gérées par la personne dirigeant l’ensemble via une console et un contrôleur MIDI. Cette personne sera au courant du 
volume de chaque interprète, de la nature de leur timbre et des effets appliqués à l’instrument en question grâce à une interface graphique. 
Cette interface réagira au changement effectué sur le contrôleur MIDI afin que les informations qui y sont affichées soit toujours à jour. La 
personne en charge de l’ensemble verra également au bon déclenchement des différentes séquences composant la pièce jouée. 



###################### Méthodes ######################

Du côté de la programmation, mon projet sera constitué de plusieurs classes gérant les instruments ainsi que les effets. Il sera également composé 
de quelques fichiers externes renfermant le code de mon interface graphique. Il y aura donc une classe pour chaque instrument, une classe pour chaque 
effet que j’aurai confectionné, une classe pour chacune des combinaisons d’effets préexistant que j’aurai combinés et finalement une classe pour la 
gestion du contrôleur avec l’interface graphique via le protocole MIDI. Les séquences d’évènements, gérant entre autres l’activation des différents 
effets en fonction du temps seront également encapsulées à l’intériesur de classes. L’ensemble de ces classes et du code de l’interface graphique 
seront tous mise en relation à l’intérieur de mon fichier principal qui se nommera « index.py ». 

L’interface graphique sera constituée des éléments suivants : une série de potentiomètres longs pour la gestion des volumes et une série de champs 
de textes affichant l’état des différents effets ainsi que la section courante. Ceci permettra au chef d’orchestre de faire l’état des informations 
pertinentes nécessaire au bon déroulement de la performance d’un simple coup d’œil. Rappelons que c’est avec un contrôleur MIDI que l’interface 
graphique sera mise en relation avec le programme audio.



###################### Implémentation ######################

- 21 Janvier 2019: Mise en place de la base de l'interface graphique en étudiant la documentation disponible sur l'api de wxPython et en faisant 
la lecture des notes de cours des deux chapitres portant sur ce sujet.

- 27 janvier 2019: reproduction de l'algorithme de 'vent dans les feuilles' d'Andy Farnell en pure data et tentative de reproduction de celui-ci 
dans python avec pyo

- 28 janvier 2019: 'vent dans les feuilles' fonctionnel en python et reproduction du script de pluie sur matière rigide d'Andy Farnell en 
Pure Data.

- 31 janvier 2019: travail sur la reproduction en python du script de la pluie.

- 04 février 2019: testes sur la communication entre une plaquette Arduino et pyhton/pyo via le port série. Plus travail sur l'interface graphique.

- 07 février 2019: Ajout & étude de la version améliorée du script de la pluie en python par Olivier Bélanger.

- 11 février 2019: Avancement sur l'interface graphique (ajout de slider, définissions des sections et besoins)

- 12 février 2019: Ménage dans le fichier de l'interface utilisateur (enlever lignes et commentaires superflus, etc.). Plus premier test 
d'import du UI à l'intérieur du fichier principal index.py

- 13 février 2019: Modification des fichiers 'feuilles' et 'pluie' afin d'en faire des class. Première tentative de reproduction en python du 
script de 'feu' d'Andy Farnell (l'ensemble des exemples d'Andy Farnell en pure data m'ont été donnée, je n'ai donc plus besoin de les reproduire 
en PD dans un premier temps).

- 18 février 2019: Travail sur l'import de l'interface graphique dans index.py. De plus, le script de 'feu' fonctionne désormais, mais il 
reste du travail à faire pour que celui-ci soit convaincant.

- 21 février 2019: Succès dans l'importation de l'interface graphique dans index.py. Reste à éclaircir quelques points pour qu'elle réagisse
bien avec le restant du code. Ménage du projet et mise à jour du plan de travail en vue de l'évaluation de lundi.

- 25 février 2019: Changement majeure dans l'architecture de mon programme. 'Index.py' ne sera finalement pas utilisé. Les intruments seront 
gérés via un objet 'audio' à partir du fichier ui.py.

- 06 mars 2019 : Modifications mineures sur l'interface graphique.

- 12 mars 2019 : Programmation de la gestion des instruments via l'interface graphique. (communication : ui --> audio --> instruments)

- 19 mars 2019 : Ajout de la gestion du volume des intruments Pluie et feuilles. Debut de l'implémentation de la gestion des volumes de tous 
les instruments via l'interface graphique. Debut des essaies avec la réception de données par le protocole OSC.

- 21 mars 2019 : Ajout de l'instrument 'fatBass' provenant des pyoTools. Finalisation de la sélection des intruments à partir de l'interface 
graphique. Travail sur la gestion des volumes via l'interface graphique.

- 25 mars 2019 : Finalisation de la gestion du volume via l'interface graphique. Courbes de volume de lin à log. Début de la gestion des effets 
(activation) via l'interface graphique.

- 26 mars 2019 : Petite modification par rapport au travail de la veile. Réception de données par OSC!

- 28 mars 2019 : Base de la gestion des effects (activation) fait. Début du mappage des données OSC sur le contrôle des effets.

- 01 avril 2019: Début de l'utilisation de 'scale' pour le mappage des données OSC. Modification Sur l'instrument feu, les quatres filtres son 
désormaient fusionné. Ajout d'effet disponible dans le selector. Ménage dans les commentaires. 

- 04 avril 2019 : Base du mapping sur les instruments, la position de l'IMU connecté au Feather affecte tous les instruments. 

- 08 avril 2019 : Mise à jour des notes concernant l'interface graphique. Début de la gestion de la calibration. Modification sur l'interface 
graphique. Début de la gestion du volume via un contrôlleur MIDI

- 11 avril 2019 : Travail sur la gestion du volume via un contrôleur MIDI. 

- 14 avril 2019 : Fine tuning du timbre de l'instrument 'Fatbass'

- 15 avril 2019 : Finalisation de la gestion du volume via un contrôleur midi.



###################### Test et maintenance ######################

- Sonorité: Une fois que les scripts ont été mis en place, j'ai fait plusieurs tests afin de déterminer quels paramètres seraient propices à 
être influencés par les données provenant de mon Arduino. J'ai par la suite fait des essais afin de rendre les timbres de mes instruments davantage 
'naturels', donc plus réalistes. Je me penche actuellement sur la conceptualisation d'un son de synthèse riche et massif afin de venir agrémenter 
mes sons d'allure naturelle. Cette étape est encore au point embryonnaire.    



- Interface graphique: Plusieurs tests ont été faits, surtout sur papier, afin de déterminer l'emplacement des différents éléments sur l'interface 
graphique. Suite à l'implémentation de la plupart des fonctionnalités pour le premier instrumentiste, j'ai ensuite concentré mes efforts sur 
l'importation de l'interface à l'intérieur de mon fichier index.py. La prochaine étape sera de faire fonctionner toutes les fonctionnalistes de ce 
premier instrument (choix du timbre, effet, volume et autre) et par la suite de recopier le code afin de mettre en place l'ensemble de l'interface. 
Dans le futur, je regarderai le tout avec un de mes amis qui est designer graphique, pour que le produit final ait une allure professionnelle.



- Communication avec Arduino: j'ai tout d'abord testé la communication via le port série, mais à cause d'un problème avec l'installation de 'pip' 
sur mon ordinateur le processus a pris beaucoup plus de temps que prévu, cependant le tout semble désormais fonctionnel. Par la suite, après avoir 
discuté avec Patrick, Martin et Olivier, j'ai décidé de mettre de côté la communication avec le port série et de faire mon projet en utilisant 
de l'OSC. Ceci permettra entre autres à mes dispositifs d'être sans fil, ce qui est un net avantage. J'attends en ce moment l'arrivée des pièces 
que j'ai commandées afin de construire mon premier dispositif et de commencer à faire mes tests.



- Communication entre python/pyo et contrôleur MIDI: Outre la lecture des notes de cours et un test sommaire, je n'ai pas avancé beaucoup sur cet 
aspect du projet. Pour le moment, je me concentre sur le bon fonctionnement de tous les éléments, une fois que tout fonctionnera bien, j'ajouterai 
la possibilité de contrôler l'interface avec un contrôleur MIDI.


** 28 avril 2019 **

Les données arrivent entre -Pi et Pi. Les données sont ensuite map entre -180 et 180, avec la fonction math.degrees(). Je peux utiliser les données 
tel quel ou effectuer un autre mappage afin d’affecter des paramètres pour lequel l’ambitus -180 à 180 serait trop grand. Possibilité de mettre les 
données en positif grâce à la fonction abs().

Pour les effets :
La position en ‘yaw’ (axe des x), affecte l’attribut ‘voice’ d’un objet pyo ‘Selector’. Objet qui fait l’interpolation entre les différents objets 
sonore qui lui sont passés en argument (dry, disto, reverb, delai, harmonizer, chorus).

Pour les instruments :
Feuilles vent : En changeant la valeur des axe ‘Pitch’ et ‘Roll’. Change la fréquence (en Hz) des deux OSC qui gère l’apparition et la répétition 
des ‘bourrasque de vent ‘.
Pluie : Change la quantité de pluie en fonction de la position en y.
Feu : Les quatre feux filtrés passe dans un dernier filtre. La position en y change la position de la bande de ce dernier filtre et le roll augmente 
la résonance (Q) du filtre.
Fat Bass : La position de l’IMU en y change le pitch de l’instrument de la manière suivante : Hz fondamental = 40 + (40*données du capteur). Données 
capteur = abs(PitchInDeg)/25

Récapitulatif :
Outre la gestion des séquences, j’ai pu mettre en place toutes les composantes que j’avais prévus en début de session dans mon plan de travail. Je 
suis donc très satisfait de mon avancement et j’attends en ce moment la réponse d’une demande de financement afin de pourvoir continuer de travailler 
sur ce projet cet été. Voici la liste des choses sur lesquelles je vais me pencher cet été :
- Le timbre des instruments pour les rendre plus convaincant.
- Avoir l’option de calibrer l’appareil.
- Possibilité de changer les instruments via un contrôleur MIDI.
- Régler le bug qui survient lorsque je change d’instrument pendant que les effets sont à ‘on’
- Implémentation des séquences ou gestion prédéfini des effets et des instruments sur une durée pré déterminé. 
- Possibilité d’avoir plusieurs effets et instruments en même temps sur une même tranche de l’interface graphique.



############# Commentaires ###############

Petit conseil, travaille avec peu de sonorités et d'effets pour commencer. Assures-toi que la communication fonctionne et que le système fait tout ce dont tu as besoin. Après, il sera facile de compléter la liste des sources et des effets.


